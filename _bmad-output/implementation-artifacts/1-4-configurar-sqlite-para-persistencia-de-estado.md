# Story 1.4: Configurar SQLite para Persist√™ncia de Estado

Status: done

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

Como Rodrigo,
Eu quero um banco SQLite para persistir o estado do pipeline e m√©tricas,
Para que o progresso seja salvo e eu possa retomar de onde parei em caso de falha (NFR11, NFR12).

## Acceptance Criteria

**Given** que os modelos Pydantic est√£o definidos
**When** implemento a camada de persist√™ncia SQLite em src/construtor/metrics/store.py
**Then** o m√≥dulo metrics/store.py cria um banco SQLite em output/pipeline_state.db
**And** a tabela `questions` √© criada com colunas mapeadas aos campos de QuestionRecord
**And** a coluna id √© INTEGER PRIMARY KEY AUTOINCREMENT
**And** colunas de timestamp usam formato ISO 8601 string
**And** colunas booleanas usam INTEGER (0/1)
**And** a coluna status usa TEXT com CHECK constraint (status IN ('pending','approved','rejected','failed'))
**And** a tabela `metrics` √© criada para armazenar QuestionMetrics por quest√£o
**And** a tabela `checkpoints` √© criada para armazenar CheckpointResult
**And** todas as escritas s√£o at√¥micas usando transa√ß√µes SQLite
**And** o m√≥dulo fornece os seguintes m√©todos p√∫blicos:
- **Questions:** save_question(), update_question_status(), get_question_by_id(), get_questions_by_status()
- **Metrics:** save_metrics(), get_metrics_by_question_id(), get_aggregate_metrics()
- **Checkpoints:** save_checkpoint(), get_checkpoint_by_id(), get_all_checkpoints()
- **Batch Progress:** save_batch_progress(), get_batch_progress()
- **Balancer State:** save_balancer_state(), get_balancer_state()
- **Resource Management:** close() (context manager support via __enter__/__exit__)
**And** nomenclatura segue snake_case para tabelas (plural) e colunas
**And** escrita n√£o bloqueia o pipeline (NFR4)

## Tasks / Subtasks

### Task 1: Design SQLite Schema (AC: #2, #3, #4, #5, #6, #11)
- [x] Analyze QuestionRecord Pydantic model from models/question.py
- [x] Map all 26 Excel columns to SQLite column types
- [x] Design `questions` table schema with proper constraints
- [x] Design `metrics` table schema for QuestionMetrics
- [x] Design `checkpoints` table schema for CheckpointResult
- [x] Add `balancer_state` table for position balancing persistence
- [x] Document all foreign key relationships
- [x] Ensure snake_case naming for all tables and columns

### Task 2: Implement Database Initialization (AC: #1)
- [x] Create MetricsStore class in src/construtor/metrics/store.py
- [x] Implement __init__() to create output/ directory if needed
- [x] Connect to output/pipeline_state.db using sqlite3
- [x] Implement _create_tables() method with all schemas
- [x] Add CREATE TABLE IF NOT EXISTS for idempotency
- [x] Enable WAL mode for concurrent reads (PRAGMA journal_mode=WAL)
- [x] Add proper error handling for database creation

### Task 3: Implement Questions Table Operations (AC: #2, #9, #10)
- [x] Implement save_question(question_record: QuestionRecord) -> int
- [x] Use INSERT with RETURNING id for new questions
- [x] Convert Pydantic model to dict for insertion
- [x] Handle timestamp serialization (ISO 8601)
- [x] Implement update_question_status(question_id: int, status: str) -> None
- [x] Use UPDATE with WHERE id = ? for status changes
- [x] Implement get_questions_by_status(status: str) -> List[QuestionRecord]
- [x] Query with WHERE status = ? and reconstruct Pydantic models
- [x] Implement get_question_by_id(question_id: int) -> QuestionRecord | None

### Task 4: Implement Metrics Table Operations (AC: #7, #10)
- [x] Implement save_metrics(question_id: int, metrics: QuestionMetrics) -> None
- [x] Use INSERT with question_id as foreign key
- [x] Store all metric fields (tokens, cost, latency, model, etc.)
- [x] Implement get_metrics_by_question_id(question_id: int) -> QuestionMetrics | None
- [x] Implement get_aggregate_metrics() -> Dict for dashboard
- [x] Calculate: total_cost, avg_latency, total_tokens, approval_rate

### Task 5: Implement Checkpoints Table Operations (AC: #8, #10)
- [x] Implement save_checkpoint(checkpoint: CheckpointResult) -> int
- [x] Store checkpoint_id, focus_range, statistics, sample_question_ids
- [x] Implement get_checkpoint_by_id(checkpoint_id: int) -> CheckpointResult | None
- [x] Implement get_all_checkpoints() -> List[CheckpointResult]
- [x] Order by timestamp DESC for latest-first

### Task 6: Implement Batch Progress Tracking (AC: #10)
- [x] Implement save_batch_progress(state: BatchState) -> None
- [x] Store current_focus, current_subfocus, total_processed
- [x] Implement get_batch_progress() -> BatchState | None
- [x] Return latest batch state for pipeline resumption
- [x] Use UPSERT (INSERT OR REPLACE) for single-row state table

### Task 7: Implement Balancer State Persistence
- [x] Implement save_balancer_state(counts: Dict[str, int]) -> None
- [x] Store counts for A, B, C, D positions
- [x] Implement get_balancer_state() -> Dict[str, int] | None
- [x] Return position counts for StatisticalBalancer recovery

### Task 8: Ensure Atomic Writes with Transactions (AC: #9)
- [x] Wrap all write operations in try/except with transactions
- [x] Use connection.execute("BEGIN TRANSACTION") for all writes
- [x] Commit only after successful write (connection.commit())
- [x] Rollback on any error (connection.rollback())
- [x] Add logging for transaction success/failure
- [x] Verify no partial writes possible (NFR12)

### Task 9: Create Comprehensive Tests
- [x] Create tests/test_metrics/test_store.py
- [x] Test database creation and schema initialization
- [x] Test save_question() with complete QuestionRecord
- [x] Test update_question_status() for all valid transitions
- [x] Test get_questions_by_status() returns correct filtered results
- [x] Test save_metrics() and get_metrics_by_question_id()
- [x] Test save_checkpoint() and checkpoint retrieval
- [x] Test batch progress save/load
- [x] Test balancer state persistence
- [x] Test atomic writes (transaction rollback on error)
- [x] Test concurrent reads don't block (WAL mode)
- [x] Use temporary database for tests (:memory: or tmpdir)

### Task 10: Validate Non-Blocking Writes (AC: #12, NFR4)
- [x] Verify WAL mode enabled (allows concurrent reads during writes)
- [x] Test async save operations don't block pipeline
- [x] Measure write latency (should be <10ms for single question)
- [x] Document performance characteristics

### Task 11: Quality Validation
- [x] Run `uv run ruff check .` and fix all warnings
- [x] Run `uv run ruff format .` for consistent formatting
- [x] Run `uv run pytest tests/test_metrics/` and verify 100% pass
- [x] Verify imports: `from construtor.metrics import MetricsStore`
- [x] Check snake_case naming throughout (tables, columns, methods)

## Dev Notes

### Context & Business Value

Esta √© a **Story 1.4** - a quarta hist√≥ria fundamental do Epic 1. Esta story cria a **camada de persist√™ncia SQLite** que permite ao pipeline salvar progresso e m√©tricas, possibilitando recupera√ß√£o ap√≥s falhas e alimentando o dashboard de monitoramento.

### Enhancement: Tabela balancer_state

**Adicionada Durante Implementa√ß√£o (n√£o nos ACs originais):**

Durante a implementa√ß√£o, foi identificada a necessidade de persistir o estado do `StatisticalBalancer` (usado para equilibrar posi√ß√µes A/B/C/D das respostas corretas). A tabela `balancer_state` foi adicionada para:

- **Persistir contadores:** Armazena contagens de quantas vezes cada posi√ß√£o (A, B, C, D) foi usada como resposta correta
- **Recupera√ß√£o ap√≥s crash:** Permite ao balancer retomar de onde parou sem perder o hist√≥rico de distribui√ß√£o
- **Conformidade NFR11:** Garante que a distribui√ß√£o estat√≠stica de respostas seja mantida mesmo ap√≥s falhas

**Schema:**
```sql
CREATE TABLE balancer_state (
    id INTEGER PRIMARY KEY CHECK(id = 1),  -- Single row table
    position_a INTEGER NOT NULL DEFAULT 0,
    position_b INTEGER NOT NULL DEFAULT 0,
    position_c INTEGER NOT NULL DEFAULT 0,
    position_d INTEGER NOT NULL DEFAULT 0,
    updated_at TEXT NOT NULL DEFAULT (datetime('now'))
)
```

**M√©todos:** `save_balancer_state(counts: Dict[str, int])`, `get_balancer_state() -> Dict[str, int] | None`

Esta adi√ß√£o **N√ÉO altera** os requisitos originais da story, mas **complementa** a funcionalidade de persist√™ncia necess√°ria para o pipeline completo.

### Import√¢ncia Cr√≠tica

**Por que esta Story √© CR√çTICA:**
1. **Resili√™ncia do Pipeline:** Sem persist√™ncia, qualquer falha durante produ√ß√£o em massa perde TODO o progresso (~8.000 quest√µes)
2. **Funda√ß√£o do Dashboard:** Epic 4 inteiro depende dos dados persistidos aqui (m√©tricas, checkpoints, progresso)
3. **Recupera√ß√£o de Custo:** Se o pipeline crashar ap√≥s US$ 500 em chamadas API, precisamos retomar sem reprocessar
4. **Checkpoint H√≠brido:** Detectar erros sist√™micos (3+ falhas) requer hist√≥rico persistido de tentativas
5. **Conformidade NFR:** NFR11 (retomada exata) e NFR12 (crash n√£o corrompe dados) dependem de escrita at√¥mica

**ERROS COMUNS A PREVENIR:**
- ‚ùå **Esquecer transa√ß√µes** ‚Üí Writes parciais corrompem banco em caso de crash
- ‚ùå **N√£o usar WAL mode** ‚Üí Writes bloqueiam reads, travando dashboard durante produ√ß√£o
- ‚ùå **Schema incompat√≠vel com Pydantic** ‚Üí Perda de dados ao converter entre modelo e tabela
- ‚ùå **Hardcoded paths** ‚Üí Quebraria em ambientes diferentes ou testes
- ‚ùå **Timestamps sem timezone** ‚Üí Bugs sutis em an√°lise de m√©tricas
- ‚ùå **Foreign keys sem ON DELETE** ‚Üí Registros √≥rf√£os acumulam no banco

### Impacto no Neg√≥cio

- **Prazo Garantido:** Recupera√ß√£o ap√≥s falhas evita perder dias reprocessando (~14/02/2026 deadline apertado)
- **Visibilidade Total:** Dashboard em tempo real permite ajustes durante produ√ß√£o (n√£o apenas post-mortem)
- **Otimiza√ß√£o de Custo:** M√©tricas persistidas permitem comparar modelos e escolher melhor custo-benef√≠cio
- **Qualidade Assegurada:** Hist√≥rico de retries e checkpoints identifica padr√µes de falha antes de escalar

### Depend√™ncias

**Bloqueado Por:**
- ‚úÖ Story 1.1 (estrutura do projeto) - COMPLETADA
- ‚úÖ Story 1.2 (modelos Pydantic) - COMPLETADA - Fornece QuestionRecord, QuestionMetrics, CheckpointResult, BatchState
- ‚ö†Ô∏è Story 1.3 (providers LLM) - COMPLETADA - M√©tricas de custo/tokens v√™m dos providers

**Bloqueia:**
- Story 1.5 (Excel reader) - precisar√° da persist√™ncia para validar focos processados
- Story 1.6 (Excel writer) - l√™ quest√µes aprovadas do SQLite para exportar
- Todas as stories de Pipeline (Epic 2) - dependem de save_question() e update_status()
- Todo o Epic 3 (Produ√ß√£o em Massa) - checkpoints e recupera√ß√£o s√£o core
- Todo o Epic 4 (Dashboard) - 100% dos dados vem do SQLite

### M√©tricas de Sucesso

- ‚úÖ Banco criado automaticamente em output/pipeline_state.db
- ‚úÖ Schema com 4 tabelas (questions, metrics, checkpoints, balancer_state)
- ‚úÖ Todas as 26 colunas do QuestionRecord mapeadas corretamente
- ‚úÖ WAL mode habilitado (PRAGMA journal_mode=WAL retorna 'wal')
- ‚úÖ Transa√ß√µes at√¥micas: rollback em erro, commit apenas em sucesso
- ‚úÖ Write latency < 10ms por quest√£o (medido em teste de performance)
- ‚úÖ Concurrent reads funcionam durante writes (WAL permite)
- ‚úÖ `ruff check` passa com zero warnings
- ‚úÖ 100% cobertura de testes para todos os m√©todos p√∫blicos
- ‚úÖ Teste de crash recovery: interromper write n√£o corrompe banco

## Technical Requirements

### Core Technologies

| Tecnologia | Vers√£o | Prop√≥sito | Documenta√ß√£o |
|-----------|---------|-----------|--------------|
| **sqlite3** | Python 3.11+ stdlib | Banco relacional embutido, zero setup | [SQLite Docs](https://docs.python.org/3/library/sqlite3.html) |
| **Pydantic** | 2.10.4+ | Serializa√ß√£o modelo ‚Üî dict para SQLite | [Pydantic Docs](https://docs.pydantic.dev/) |
| **pathlib** | Python 3.11+ stdlib | Manipula√ß√£o de paths cross-platform | [Pathlib Docs](https://docs.python.org/3/library/pathlib.html) |

### SQLite Latest Best Practices (2026)

**WAL Mode (Write-Ahead Logging):**
```sql
PRAGMA journal_mode=WAL;
```
- **Benef√≠cio:** Permite leituras concorrentes durante writes (dashboard n√£o trava durante produ√ß√£o)
- **Como funciona:** Writes v√£o para arquivo .wal separado, readers continuam lendo arquivo principal
- **Performance:** ~50% faster para workloads com reads frequentes
- **Habilita√ß√£o:** Uma vez por conex√£o, persistido no arquivo .db

**Strict Mode (SQLite 3.37+):**
```sql
PRAGMA strict=ON;
```
- **Benef√≠cio:** Type safety - INT s√≥ aceita integers, TEXT s√≥ aceita strings
- **Previne:** Bugs sutis onde Python insere "123" em coluna INT e SQLite aceita silenciosamente
- **Recomenda√ß√£o:** Usar em produ√ß√£o para detectar erros de tipo cedo

**Foreign Keys (Habilita√ß√£o Obrigat√≥ria):**
```sql
PRAGMA foreign_keys=ON;
```
- **Benef√≠cio:** Garante integridade referencial (question_id em metrics deve existir em questions)
- **Default:** OFF (por compatibilidade legacy) - DEVE ser habilitado explicitamente
- **Cascading:** ON DELETE CASCADE garante limpeza autom√°tica de dados relacionados

**CHECK Constraints para Enums:**
```sql
status TEXT CHECK(status IN ('pending','approved','rejected','failed'))
```
- **Benef√≠cio:** Valida valores permitidos no n√≠vel do banco (defesa em profundidade)
- **Erro antecipado:** INSERT com status='wrong' falha imediatamente, n√£o propaga

### Schema Design Patterns

**Padr√£o de Nomenclatura (Arquitetura Mandat√≥ria):**
```python
# Tabelas: snake_case, PLURAL
CREATE TABLE questions (...);
CREATE TABLE metrics (...);
CREATE TABLE checkpoints (...);

# Colunas: snake_case
id INTEGER PRIMARY KEY AUTOINCREMENT
question_id INTEGER
created_at TEXT
modelo_llm TEXT  -- Portugu√™s para campos de dom√≠nio
```

**Timestamps em ISO 8601 (UTC):**
```python
from datetime import datetime, timezone

# SEMPRE usar UTC para timestamps
timestamp = datetime.now(timezone.utc).isoformat()
# Resultado: "2026-02-07T22:30:00+00:00"

# No SQLite
CREATE TABLE questions (
    created_at TEXT NOT NULL DEFAULT (datetime('now')),
    updated_at TEXT
);
```

**Boolean como INTEGER (0/1):**
```sql
concordancia_comentador INTEGER DEFAULT 0 CHECK(concordancia_comentador IN (0, 1))
```
- SQLite n√£o tem tipo BOOLEAN nativo
- Convention: 0 = False, 1 = True
- CHECK constraint previne valores inv√°lidos (2, -1, NULL)

**JSON para Arrays/Dicts:**
```sql
sample_question_ids TEXT  -- Stored as JSON: "[1, 2, 3, 4, 5]"
```
```python
import json

# Save
ids_json = json.dumps([1, 2, 3, 4, 5])
cursor.execute("INSERT INTO checkpoints (sample_question_ids) VALUES (?)", (ids_json,))

# Load
row = cursor.fetchone()
ids = json.loads(row['sample_question_ids'])  # [1, 2, 3, 4, 5]
```

### Atomic Writes Pattern (CRITICAL)

**Transaction Pattern (ALWAYS):**
```python
import sqlite3
import logging

logger = logging.getLogger(__name__)

def save_question(self, question: QuestionRecord) -> int:
    """
    Save question with atomic transaction.

    CRITICAL: ALL writes MUST use this pattern to prevent corruption (NFR12).
    """
    try:
        # BEGIN TRANSACTION (implicit with cursor)
        cursor = self.conn.cursor()

        # Convert Pydantic to dict
        data = question.model_dump()

        # INSERT with RETURNING
        cursor.execute("""
            INSERT INTO questions (tema, foco, sub_foco, ...)
            VALUES (:tema, :foco, :sub_foco, ...)
            RETURNING id
        """, data)

        question_id = cursor.fetchone()[0]

        # COMMIT only if successful
        self.conn.commit()

        logger.info(f"Question {question_id} saved successfully")
        return question_id

    except sqlite3.Error as e:
        # ROLLBACK on any error
        self.conn.rollback()
        logger.error(f"Failed to save question: {e}", exc_info=True)
        raise PipelineError(f"Database write failed: {e}") from e
```

**Por que isso √© CR√çTICO:**
- Sem transaction: crash durante INSERT deixa dados parciais
- Sem rollback: erro propagado com dados inconsistentes no banco
- Sem commit expl√≠cito: autocommit pode commitar em momentos errados

### Pydantic ‚Üî SQLite Conversion

**Salvar (Pydantic ‚Üí SQLite):**
```python
from construtor.models import QuestionRecord

question = QuestionRecord(
    tema="Cardiologia",
    foco="Insufici√™ncia Card√≠aca",
    # ... outros campos
)

# Converter para dict (keys = nomes dos campos)
data = question.model_dump()
# {'tema': 'Cardiologia', 'foco': 'Insufici√™ncia Card√≠aca', ...}

# INSERT com named parameters (:tema, :foco, ...)
cursor.execute("""
    INSERT INTO questions (tema, foco, sub_foco, ...)
    VALUES (:tema, :foco, :sub_foco, ...)
""", data)
```

**Carregar (SQLite ‚Üí Pydantic):**
```python
# Query retorna Row object (dict-like)
cursor.execute("SELECT * FROM questions WHERE id = ?", (question_id,))
row = cursor.fetchone()

# Converter Row para dict
data = dict(row)

# Reconstruir Pydantic model
question = QuestionRecord(**data)
# Valida√ß√£o autom√°tica acontece aqui!
```

**IMPORTANTE:** Nomes de colunas SQLite devem ser ID√äNTICOS aos campos Pydantic.

### Performance Considerations

**√çndices para Queries Frequentes:**
```sql
-- Dashboard queries filtram por status MUITO frequentemente
CREATE INDEX idx_questions_status ON questions(status);

-- M√©tricas agregadas agrupam por modelo
CREATE INDEX idx_metrics_model ON metrics(modelo_llm);

-- Checkpoints ordenados por timestamp
CREATE INDEX idx_checkpoints_timestamp ON checkpoints(created_at DESC);
```

**Connection Pooling N√ÉO Necess√°rio:**
- Single-user application, execu√ß√£o local
- Uma conex√£o persistente por processo √© suficiente
- WAL mode j√° permite concurrent reads

**Write Batching para Performance:**
```python
def save_questions_batch(self, questions: List[QuestionRecord]) -> None:
    """Save multiple questions in single transaction (faster)."""
    try:
        cursor = self.conn.cursor()
        for question in questions:
            data = question.model_dump()
            cursor.execute("INSERT INTO questions (...) VALUES (...)", data)
        self.conn.commit()  # Single commit for entire batch
    except sqlite3.Error as e:
        self.conn.rollback()
        raise PipelineError(f"Batch save failed: {e}") from e
```

## Architecture Compliance

### Naming Conventions (CRITICAL - Must Follow Exactly)

**Fonte:** [Architecture Doc - Naming Patterns](../../_bmad-output/planning-artifacts/architecture.md#naming-patterns)

| Elemento | Conven√ß√£o | Exemplos | Idioma |
|---------|-----------|----------|--------|
| **Tabelas** | snake_case, PLURAL | `questions`, `metrics`, `checkpoints` | Ingl√™s t√©cnico |
| **Colunas** | snake_case | `question_id`, `created_at`, `tokens_used` | Ingl√™s t√©cnico |
| **Colunas de Dom√≠nio** | snake_case | `tema`, `foco`, `sub_foco`, `enunciado` | **Portugu√™s** (mapeia Excel) |
| **Classes** | PascalCase | `MetricsStore`, `QuestionRecord` | Ingl√™s |
| **M√©todos** | snake_case | `save_question()`, `get_batch_progress()` | Ingl√™s |
| **Vari√°veis** | snake_case | `question_id`, `cursor`, `conn` | Ingl√™s |

**IMPORTANTE:** Colunas mapeadas aos 26 campos do Excel devem manter nomes em PORTUGU√äS (tema, foco, enunciado, alternativa_a, etc.) para consist√™ncia com QuestionRecord.

### Mandatory Database Patterns

**Fonte:** [Architecture Doc - Structure Patterns](../../_bmad-output/planning-artifacts/architecture.md#structure-patterns)

**TODA opera√ß√£o de escrita DEVE seguir este padr√£o:**

```python
import sqlite3
from pathlib import Path
from construtor.config.exceptions import PipelineError

class MetricsStore:
    """SQLite persistence layer for pipeline state and metrics."""

    def __init__(self, db_path: str = "output/pipeline_state.db"):
        """
        Initialize database connection.

        Args:
            db_path: Path to SQLite database file (created if not exists)
        """
        # Create output directory if needed
        db_file = Path(db_path)
        db_file.parent.mkdir(parents=True, exist_ok=True)

        # Connect to database
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row  # Dict-like access to rows

        # Enable critical PRAGMAs
        self.conn.execute("PRAGMA journal_mode=WAL")
        self.conn.execute("PRAGMA foreign_keys=ON")

        # Create tables
        self._create_tables()

    def _create_tables(self) -> None:
        """Create all tables with proper schema."""
        cursor = self.conn.cursor()

        # Questions table (maps to QuestionRecord)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS questions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                tema TEXT NOT NULL,
                foco TEXT NOT NULL,
                sub_foco TEXT NOT NULL,
                -- ... all 26 columns
                status TEXT NOT NULL DEFAULT 'pending'
                    CHECK(status IN ('pending','approved','rejected','failed')),
                created_at TEXT NOT NULL DEFAULT (datetime('now')),
                updated_at TEXT
            )
        """)

        # Metrics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question_id INTEGER NOT NULL,
                modelo_llm TEXT NOT NULL,
                tokens_used INTEGER NOT NULL,
                cost REAL NOT NULL,
                latency REAL NOT NULL,
                rodadas INTEGER DEFAULT 1,
                created_at TEXT NOT NULL DEFAULT (datetime('now')),
                FOREIGN KEY (question_id) REFERENCES questions(id) ON DELETE CASCADE
            )
        """)

        # Create indexes
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_questions_status ON questions(status)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_metrics_model ON metrics(modelo_llm)")

        self.conn.commit()
```

### Anti-Patterns (STRICTLY FORBIDDEN)

**Fonte:** [Architecture Doc - Anti-Patterns](../../_bmad-output/planning-artifacts/architecture.md#anti-patterns-proibidos)

```python
# ‚ùå ANTI-PATTERN 1: Write sem transaction
def save_question(self, question):
    cursor = self.conn.cursor()
    cursor.execute("INSERT INTO questions (...) VALUES (...)", data)
    # Crash aqui = dados corrompidos!
    self.conn.commit()

# ‚úÖ CORRETO: Transaction com try/except
def save_question(self, question):
    try:
        cursor = self.conn.cursor()
        cursor.execute("INSERT INTO questions (...) VALUES (...)", data)
        self.conn.commit()
    except sqlite3.Error as e:
        self.conn.rollback()
        raise PipelineError(f"Write failed: {e}") from e


# ‚ùå ANTI-PATTERN 2: Concatena√ß√£o de SQL (SQL injection risk)
def get_by_status(self, status):
    query = f"SELECT * FROM questions WHERE status = '{status}'"  # Perigoso!
    return self.conn.execute(query).fetchall()

# ‚úÖ CORRETO: Parameterized queries
def get_by_status(self, status):
    return self.conn.execute(
        "SELECT * FROM questions WHERE status = ?",
        (status,)
    ).fetchall()


# ‚ùå ANTI-PATTERN 3: Ignorar WAL mode
conn = sqlite3.connect("output/pipeline_state.db")
# Writes bloqueiam reads! Dashboard trava durante produ√ß√£o.

# ‚úÖ CORRETO: Habilitar WAL mode
conn = sqlite3.connect("output/pipeline_state.db")
conn.execute("PRAGMA journal_mode=WAL")


# ‚ùå ANTI-PATTERN 4: Timestamps naive (sem timezone)
from datetime import datetime
timestamp = datetime.now().isoformat()  # Sem timezone! Bugs em DST.

# ‚úÖ CORRETO: UTC expl√≠cito
from datetime import datetime, timezone
timestamp = datetime.now(timezone.utc).isoformat()
```

## Library & Framework Requirements

### SQLite Python (stdlib)

**Vers√£o:** Python 3.11+ (SQLite 3.37+)

**Imports Necess√°rios:**
```python
import sqlite3
from pathlib import Path
from datetime import datetime, timezone
import json
import logging
```

**Connection Configuration:**
```python
conn = sqlite3.connect(
    "output/pipeline_state.db",
    check_same_thread=False  # Allow multi-thread access (safe with GIL)
)
conn.row_factory = sqlite3.Row  # Dict-like row access
```

**Critical PRAGMAs:**
```python
conn.execute("PRAGMA journal_mode=WAL")  # Concurrent reads during writes
conn.execute("PRAGMA foreign_keys=ON")   # Referential integrity
conn.execute("PRAGMA synchronous=NORMAL")  # Balance safety/speed
```

### Pydantic Integration

**Models to Persist:**
```python
from construtor.models import (
    QuestionRecord,      # 26-column model (maps to questions table)
    QuestionMetrics,     # Metrics per question (maps to metrics table)
    CheckpointResult,    # Checkpoint data (maps to checkpoints table)
    BatchState,          # Batch progress (maps to batch_state table)
)
```

**Serialization Pattern:**
```python
# Pydantic ‚Üí SQLite
data = model.model_dump()  # Returns dict
cursor.execute("INSERT INTO table (...) VALUES (:field1, :field2, ...)", data)

# SQLite ‚Üí Pydantic
row = cursor.fetchone()
model = QuestionRecord(**dict(row))  # Validation happens here
```

### Testing Dependencies

**Pytest Fixtures for SQLite:**
```python
import pytest
import sqlite3
from pathlib import Path

@pytest.fixture
def temp_db(tmp_path):
    """Temporary database for tests."""
    db_path = tmp_path / "test.db"
    store = MetricsStore(str(db_path))
    yield store
    store.conn.close()

@pytest.fixture
def in_memory_db():
    """In-memory database (faster, but no file I/O testing)."""
    store = MetricsStore(":memory:")
    yield store
    store.conn.close()
```

## File Structure Requirements

### Files to Create

```
src/construtor/metrics/
‚îú‚îÄ‚îÄ __init__.py                    # Export MetricsStore
‚îî‚îÄ‚îÄ store.py                       # MetricsStore implementation

tests/test_metrics/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ test_store.py                  # Comprehensive tests (20+ tests)

output/
‚îî‚îÄ‚îÄ pipeline_state.db              # Created automatically (gitignored)
```

### MetricsStore Public API

```python
class MetricsStore:
    """SQLite persistence layer for pipeline state and metrics."""

    # Questions table
    def save_question(self, question: QuestionRecord) -> int: ...
    def update_question_status(self, question_id: int, status: str) -> None: ...
    def get_question_by_id(self, question_id: int) -> QuestionRecord | None: ...
    def get_questions_by_status(self, status: str) -> List[QuestionRecord]: ...

    # Metrics table
    def save_metrics(self, question_id: int, metrics: QuestionMetrics) -> None: ...
    def get_metrics_by_question_id(self, question_id: int) -> QuestionMetrics | None: ...
    def get_aggregate_metrics(self) -> Dict[str, Any]: ...

    # Checkpoints table
    def save_checkpoint(self, checkpoint: CheckpointResult) -> int: ...
    def get_checkpoint_by_id(self, checkpoint_id: int) -> CheckpointResult | None: ...
    def get_all_checkpoints(self) -> List[CheckpointResult]: ...

    # Batch progress table
    def save_batch_progress(self, state: BatchState) -> None: ...
    def get_batch_progress(self) -> BatchState | None: ...

    # Balancer state table
    def save_balancer_state(self, counts: Dict[str, int]) -> None: ...
    def get_balancer_state(self) -> Dict[str, int] | None: ...
```

## Testing Requirements

### Test Coverage Checklist

**tests/test_metrics/test_store.py deve incluir:**

1. **Initialization Tests (3 tests)**
   - [ ] Test database file created automatically
   - [ ] Test output/ directory created if not exists
   - [ ] Test all tables created with correct schema

2. **Questions Table Tests (8 tests)**
   - [ ] Test save_question() returns valid question_id
   - [ ] Test save_question() with all 26 QuestionRecord fields
   - [ ] Test get_question_by_id() returns correct question
   - [ ] Test get_question_by_id() returns None for invalid ID
   - [ ] Test update_question_status() changes status correctly
   - [ ] Test get_questions_by_status() filters correctly
   - [ ] Test status CHECK constraint rejects invalid status
   - [ ] Test timestamps auto-populated (created_at)

3. **Metrics Table Tests (4 tests)**
   - [ ] Test save_metrics() with foreign key to question
   - [ ] Test get_metrics_by_question_id() returns correct metrics
   - [ ] Test get_aggregate_metrics() calculates totals correctly
   - [ ] Test foreign key constraint (delete question cascades metrics)

4. **Checkpoints Table Tests (3 tests)**
   - [ ] Test save_checkpoint() with sample_question_ids as JSON
   - [ ] Test get_checkpoint_by_id() deserializes JSON correctly
   - [ ] Test get_all_checkpoints() ordered by timestamp DESC

5. **Batch Progress Tests (2 tests)**
   - [ ] Test save_batch_progress() UPSERT behavior
   - [ ] Test get_batch_progress() returns latest state

6. **Balancer State Tests (2 tests)**
   - [ ] Test save_balancer_state() with A/B/C/D counts
   - [ ] Test get_balancer_state() returns correct dict

7. **Transaction Tests (3 tests)**
   - [ ] Test successful write commits transaction
   - [ ] Test failed write rolls back (no partial data)
   - [ ] Test exception raised on database error

8. **Concurrency Tests (2 tests)**
   - [ ] Test WAL mode enabled (PRAGMA returns 'wal')
   - [ ] Test concurrent reads during write don't block

**Total: ~27 tests m√≠nimo**

### Test Pattern Example

```python
import pytest
from construtor.metrics import MetricsStore
from construtor.models import QuestionRecord

def test_save_and_retrieve_question(temp_db):
    """Test complete save/load cycle for question."""
    # Arrange
    question = QuestionRecord(
        tema="Cardiologia",
        foco="Insufici√™ncia Card√≠aca",
        sub_foco="Classifica√ß√£o NYHA",
        # ... all 26 fields
    )

    # Act
    question_id = temp_db.save_question(question)
    retrieved = temp_db.get_question_by_id(question_id)

    # Assert
    assert retrieved is not None
    assert retrieved.tema == "Cardiologia"
    assert retrieved.foco == "Insufici√™ncia Card√≠aca"
    # ... validate all fields match

def test_transaction_rollback_on_error(temp_db):
    """Test atomic writes: error causes rollback."""
    # Arrange
    initial_count = len(temp_db.get_questions_by_status('pending'))

    # Act - Force an error (invalid status)
    with pytest.raises(Exception):
        question = QuestionRecord(status='invalid_status', ...)
        temp_db.save_question(question)

    # Assert - No partial write
    final_count = len(temp_db.get_questions_by_status('pending'))
    assert final_count == initial_count
```

## Previous Story Intelligence (Story 1.3)

### Principais Aprendizados da Implementa√ß√£o da Story 1.3

**‚úÖ Qualidade de C√≥digo Estabelecida:**
- 54 testes criados e 100% passando (14 exceptions + 11 protocol + 14 OpenAI + 15 Anthropic)
- Zero erros ruff ap√≥s corre√ß√µes (originalmente 271 erros identificados e corrigidos)
- Arquivo `.ruff.toml` criado com regras espec√≠ficas para produ√ß√£o vs testes
- Pattern de commit message estabelecido com Co-Authored-By tag

**‚úÖ Patterns T√©cnicos Validados:**
- Protocol pattern funciona perfeitamente para abstra√ß√£o LLM
- Tenacity para retry exponencial com jitter √© eficaz
- AsyncIO + Semaphore para controle de concorr√™ncia testado
- Exception hierarchy reutilizada com sucesso (da Story 1-2)

**‚úÖ Arquivos Prontos para Uso:**
- `src/construtor/config/exceptions.py` ‚Üí PipelineError, LLMProviderError dispon√≠veis para import
- `src/construtor/providers/` ‚Üí OpenAIProvider, AnthropicProvider funcionais
- `src/construtor/models/` ‚Üí QuestionRecord, QuestionMetrics, CheckpointResult, BatchState prontos
- `.ruff.toml` ‚Üí Configurado e funcional

**‚úÖ Li√ß√µes Aprendidas:**
1. **TDD Approach:** Criar testes durante implementa√ß√£o (n√£o depois) garante melhor cobertura
2. **Ruff Early:** Rodar `ruff check` cedo evita ac√∫mulo de warnings (271 ‚Üí 0 foi trabalhoso)
3. **ClassVar Annotations:** Necess√°rio para dicts de classe (PRICING) evitar warnings ruff
4. **Exception Chaining:** Sempre usar `raise ... from err` para preservar stack trace original
5. **Story Documentation:** Atualizar story file com completion notes e file list durante implementa√ß√£o

**‚úÖ Padr√µes Estabelecidos para Story 1.4:**
- Usar mesmo pattern de testes: fixtures com temp_db e in_memory_db
- Seguir mesmo n√≠vel de documenta√ß√£o (docstrings completas com Args/Returns/Raises)
- Aplicar ruff desde o in√≠cio (n√£o acumular warnings)
- Documentar completion notes durante implementa√ß√£o (n√£o post-mortem)
- File list deve distinguir CRIADO/MODIFICADO/REUTILIZADO

**üîó Conex√£o Direta com Story 1.4:**
- MetricsStore precisar√° importar exce√ß√µes: `from construtor.config.exceptions import PipelineError`
- MetricsStore salvar√° custos calculados pelos providers: `cost` field em metrics table
- Pattern de teste ser√° id√™ntico: pytest fixtures, 100% coverage, ruff zero warnings
- Transa√ß√µes SQLite seguem mesmo rigor que retry logic dos providers (atomicidade cr√≠tica)

## Git Intelligence Summary

### An√°lise dos Commits Recentes

**Commit 1accb91 (Mais Recente - Story 1.3):**
```
feat: implementar abstra√ß√£o de provedores LLM com qualidade de c√≥digo

Arquivos criados/modificados:
- .ruff.toml (CRIADO)
- src/construtor/providers/base.py (CRIADO)
- src/construtor/providers/openai_provider.py (CRIADO)
- src/construtor/providers/anthropic_provider.py (CRIADO)
- src/construtor/providers/__init__.py (MODIFICADO)
- tests/test_providers/test_base.py (CRIADO)
- tests/test_providers/test_openai_provider.py (CRIADO)
- tests/test_providers/test_anthropic_provider.py (CRIADO)
```

**Padr√µes Identificados:**
1. **Estrutura de Commit:** Mensagem detalhada com bullet points explicando implementa√ß√£o + qualidade + documenta√ß√£o
2. **Co-Authored Tag:** Sempre incluir `Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>`
3. **Organiza√ß√£o de Arquivos:** Criar m√≥dulo completo por vez (base + implementations + tests + exports)
4. **Ruff Configuration:** Arquivo `.ruff.toml` centraliza regras de lint/format
5. **Test Coverage:** M√≠nimo 40+ testes por story, 100% passing antes de commit
6. **Story Updates:** Story markdown file sempre atualizado com status e completion notes

**Commit d0e53ba (Story 1.2):**
```
feat: implementar modelos Pydantic de dados para pipeline de quest√µes

Arquivos criados:
- src/construtor/models/question.py
- src/construtor/models/feedback.py
- src/construtor/models/pipeline.py
- src/construtor/models/metrics.py
- tests/test_models/ (41 testes)
```

**Padr√µes Identificados:**
1. **Modelos Pydantic:** Usar `ConfigDict(strict=True)` em todos os models
2. **Nomenclatura Bil√≠ngue:** Campos t√©cnicos em ingl√™s, campos de dom√≠nio m√©dico em portugu√™s
3. **Field Descriptions:** Sempre incluir `Field(..., description="...")` para documenta√ß√£o
4. **Testing Pattern:** Testar valida√ß√£o Pydantic com dados v√°lidos E inv√°lidos

**Commit c04c303 (Story 1.1):**
```
feat: inicializar projeto com estrutura base completa

Estrutura criada:
- Inicializa√ß√£o com uv (Python 3.11)
- Estrutura src/construtor/ com 8 m√≥dulos
- pyproject.toml configurado
- .gitignore configurado
```

### Insights Acion√°veis para Story 1.4

**1. Estrutura de Diret√≥rio Estabelecida:**
```
src/construtor/metrics/  ‚Üê Story 1.4 vai aqui
‚îú‚îÄ‚îÄ __init__.py          ‚Üê Exportar MetricsStore
‚îî‚îÄ‚îÄ store.py             ‚Üê Implementa√ß√£o completa
```

**2. Pattern de Depend√™ncias:**
- Story 1.4 pode importar de `construtor.models` (QuestionRecord, QuestionMetrics)
- Story 1.4 pode importar de `construtor.config.exceptions` (PipelineError)
- Adicionar sqlite3 (stdlib) - zero depend√™ncias extras necess√°rias

**3. Conven√ß√µes de C√≥digo:**
- Seguir PEP 8 rigorosamente (validado por ruff)
- Docstrings completas em estilo Google (Args/Returns/Raises)
- Type hints em TODOS os m√©todos p√∫blicos
- ClassVar para constantes de classe

**4. Testing Strategy:**
- Criar tests/test_metrics/test_store.py
- Usar pytest fixtures (temp_db, in_memory_db)
- M√≠nimo 27 testes (baseado em an√°lise de cobertura)
- Validar com `uv run pytest tests/test_metrics/` antes de commit

**5. Commit Message Template:**
```
feat: implementar persist√™ncia SQLite para estado e m√©tricas

Implementa camada de persist√™ncia com WAL mode e transa√ß√µes at√¥micas:
- MetricsStore com 4 tabelas (questions, metrics, checkpoints, balancer_state)
- Schema mapeado aos 26 campos de QuestionRecord
- Transa√ß√µes at√¥micas para todas as escritas (NFR12)
- WAL mode para reads concorrentes durante writes (NFR4)
- 27+ testes com 100% coverage (todos passando)

Qualidade de c√≥digo:
- Ruff check passa com 0 warnings
- Type hints completos em todos os m√©todos
- Exception handling com PipelineError
- Documenta√ß√£o completa com docstrings

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
```

## Latest Tech Information (2026)

### SQLite 3.45.0 (Python 3.11+ stdlib)

**Vers√£o Inclu√≠da:** Python 3.11.14 includes SQLite 3.45.0 (latest stable as of Feb 2026)

**Novos Features Relevantes:**
- **STRICT Tables:** Type enforcement at database level (`PRAGMA strict=ON`)
- **RETURNING Clause:** `INSERT ... RETURNING id` sem precisar de SELECT separado
- **Improved WAL Mode:** Performance ~30% melhor que SQLite 3.37
- **JSON Functions:** `json_extract()`, `json_array()` para trabalhar com dados JSON

**WAL Mode (Write-Ahead Logging) - Cr√≠tico para NFR4:**
```sql
PRAGMA journal_mode=WAL;
```
- **Performance Gain:** Writes s√£o ~50% mais r√°pidas
- **Concurrency:** M√∫ltiplos readers simult√¢neos mesmo durante write
- **Persist√™ncia:** Uma vez habilitado, persiste no arquivo .db
- **Recomenda√ß√£o:** SEMPRE habilitar para applications com reads frequentes

**Best Practices SQLite (2026):**
```python
import sqlite3

# Connection setup
conn = sqlite3.connect("pipeline_state.db", check_same_thread=False)
conn.row_factory = sqlite3.Row  # Dict-like row access

# Critical PRAGMAs
conn.execute("PRAGMA journal_mode=WAL")      # Concurrent reads
conn.execute("PRAGMA foreign_keys=ON")       # Referential integrity
conn.execute("PRAGMA synchronous=NORMAL")    # Balance safety/speed (FULL √© muito lento)
conn.execute("PRAGMA cache_size=-64000")     # 64MB cache (default √© 2MB)
```

**Sources:**
- [SQLite Release 3.45.0](https://www.sqlite.org/releaselog/3_45_0.html)
- [Python sqlite3 Documentation](https://docs.python.org/3/library/sqlite3.html)
- [SQLite WAL Mode](https://www.sqlite.org/wal.html)

### Python 3.11 sqlite3 Module

**Type Adapters and Converters:**
```python
import sqlite3
from datetime import datetime, timezone

# Registrar adapter para datetime ‚Üí ISO 8601
sqlite3.register_adapter(datetime, lambda dt: dt.isoformat())

# Registrar converter para ISO 8601 ‚Üí datetime
sqlite3.register_converter("timestamp", lambda s: datetime.fromisoformat(s.decode()))
```

**Row Factory Patterns:**
```python
# Dict-like access (RECOMENDADO)
conn.row_factory = sqlite3.Row
row = cursor.fetchone()
print(row['tema'])  # Access by column name

# Tuple access (default)
conn.row_factory = None
row = cursor.fetchone()
print(row[0])  # Access by index
```

**Context Manager for Transactions:**
```python
# Automatic commit/rollback
with conn:
    conn.execute("INSERT INTO questions (...) VALUES (...)")
# Commits automatically if no exception, rolls back otherwise
```

**Sources:**
- [Python sqlite3 Module](https://docs.python.org/3/library/sqlite3.html)

### Pydantic 2.10 Integration with SQLite

**Model Serialization:**
```python
from pydantic import BaseModel

class QuestionRecord(BaseModel):
    tema: str
    foco: str
    # ... 26 fields total

# Serialize to dict for SQLite
data = question.model_dump()
# {'tema': 'Cardiologia', 'foco': 'ICC', ...}

# Deserialize from SQLite Row
row = cursor.fetchone()
question = QuestionRecord(**dict(row))
# Automatic validation happens here!
```

**JSON Fields:**
```python
from typing import List
from pydantic import BaseModel, Field

class CheckpointResult(BaseModel):
    sample_question_ids: List[int] = Field(default_factory=list)

# Save to SQLite
import json
ids_json = json.dumps(checkpoint.sample_question_ids)
cursor.execute("INSERT INTO checkpoints (sample_question_ids) VALUES (?)", (ids_json,))

# Load from SQLite
row = cursor.fetchone()
checkpoint = CheckpointResult(sample_question_ids=json.loads(row['sample_question_ids']))
```

**Sources:**
- [Pydantic Documentation](https://docs.pydantic.dev/latest/)
- [Pydantic Serialization](https://docs.pydantic.dev/latest/concepts/serialization/)

### Testing with pytest and sqlite3

**Fixture Patterns:**
```python
import pytest
import sqlite3
from pathlib import Path

@pytest.fixture
def temp_db(tmp_path):
    """Temporary file-based database (tests file I/O)."""
    db_path = tmp_path / "test.db"
    conn = sqlite3.connect(str(db_path))
    yield conn
    conn.close()

@pytest.fixture
def memory_db():
    """In-memory database (faster, no disk I/O)."""
    conn = sqlite3.connect(":memory:")
    yield conn
    conn.close()
```

**Parametrized Tests:**
```python
@pytest.mark.parametrize("status", ["pending", "approved", "rejected", "failed"])
def test_get_questions_by_status(temp_db, status):
    """Test filtering by all valid statuses."""
    # Test implementation
```

**Sources:**
- [pytest Documentation](https://docs.pytest.org/)
- [pytest Fixtures](https://docs.pytest.org/en/stable/how-to/fixtures.html)

## Project Structure Notes

### Alignment with Unified Project Structure

**100% Compliant with Architecture Document:**
- ‚úÖ Follows [metrics/ module structure](../../_bmad-output/planning-artifacts/architecture.md#complete-project-directory-structure)
- ‚úÖ Implements [SQLite persistence layer](../../_bmad-output/planning-artifacts/architecture.md#persist√™ncia-de-estado)
- ‚úÖ Uses [snake_case naming](../../_bmad-output/planning-artifacts/architecture.md#naming-patterns) for all tables/columns
- ‚úÖ Follows [transaction patterns](../../_bmad-output/planning-artifacts/architecture.md#process-patterns)
- ‚úÖ Implements [Boundary 4: Metrics ‚Üî Todos](../../_bmad-output/planning-artifacts/architecture.md#architectural-boundaries)

**Critical for Future Stories:**
- Story 1.5 (Excel Reader) precisar√° verificar focos j√° processados via MetricsStore
- Story 1.6 (Excel Writer) ler√° quest√µes aprovadas do SQLite para exportar
- Story 2.2 (CriadorAgent) salvar√° quest√µes geradas via save_question()
- Story 2.6 (ValidadorAgent) atualizar√° status via update_question_status()
- Story 2.7 (Retry Manager) rastrear√° tentativas no SQLite
- Story 3.2 (Checkpoints) salvar√° checkpoints via save_checkpoint()
- Story 3.4 (Recovery) carregar√° batch_progress via get_batch_progress()
- Todo Epic 4 (Dashboard) ler√° TODOS os dados do SQLite

**Detected Conflicts:** Nenhum - implementa√ß√£o greenfield alinhada com arquitetura

### References

**Architecture Documentation:**
- [SQLite Persistence Decision](../../_bmad-output/planning-artifacts/architecture.md#persist√™ncia-de-estado) - Lines 177-184
- [Metrics Store Schema](../../_bmad-output/planning-artifacts/architecture.md#structure-patterns) - SQLite patterns
- [Transaction Patterns](../../_bmad-output/planning-artifacts/architecture.md#process-patterns) - Atomic writes
- [Boundary 4: Metrics ‚Üî All](../../_bmad-output/planning-artifacts/architecture.md#architectural-boundaries) - Lines 546-551

**Requirements Documentation:**
- [PRD - State Persistence](../../_bmad-output/planning-artifacts/prd.md#resili√™ncia) - NFR11-14
- [Epics - Story 1.4](../../_bmad-output/planning-artifacts/epics.md#story-14-configurar-sqlite-para-persist√™ncia-de-estado) - Lines 258-279

**Related Stories:**
- [Story 1.1 - Project Init](./1-1-inicializar-projeto-e-estrutura-base.md) - Completada (estrutura criada)
- [Story 1.2 - Pydantic Models](./1-2-criar-modelos-pydantic-de-dados.md) - Completada (modelos dispon√≠veis)
- [Story 1.3 - LLM Providers](./1-3-implementar-abstracao-de-provedores-llm.md) - Completada (m√©tricas de custo)
- Story 1.5 - Excel Reader (bloqueada por esta story - precisa verificar progresso)
- Story 1.6 - Excel Writer (bloqueada por esta story - l√™ do SQLite)

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References

N/A - Implementation completed without blocking issues

### Completion Notes List

**Implementation Summary:**
- ‚úÖ Created MetricsStore class with 5 tables (questions, metrics, checkpoints, batch_state, balancer_state)
- ‚úÖ Implemented all 26 QuestionRecord fields mapping to questions table
- ‚úÖ Enabled WAL mode for concurrent reads during writes (NFR4 compliance)
- ‚úÖ All writes use atomic transactions with try/except/rollback pattern (NFR12 compliance)
- ‚úÖ Foreign keys enabled with CASCADE delete for referential integrity
- ‚úÖ CHECK constraints for status, difficulty, position, and range validation
- ‚úÖ Indexes created for frequent queries (status, question_id, modelo, created_at)
- ‚úÖ JSON serialization for list fields (sample_question_ids)
- ‚úÖ Boolean to INTEGER conversion for SQLite compatibility
- ‚úÖ UPSERT pattern for single-row tables (batch_state, balancer_state)

**Testing:**
- ‚úÖ 27 comprehensive tests created and all passing
- ‚úÖ Test coverage: initialization (3), questions (8), metrics (4), checkpoints (3), batch (2), balancer (2), transactions (3), concurrency (2)
- ‚úÖ Validates atomic transactions, WAL mode, foreign keys, CHECK constraints, timestamps

**Quality:**
- ‚úÖ Ruff check: 0 errors
- ‚úÖ Ruff format: All files formatted
- ‚úÖ Type hints: Complete on all public methods
- ‚úÖ Docstrings: Complete with Args/Returns/Raises
- ‚úÖ Exception handling: All writes wrapped in try/except with PipelineError

**Architecture Compliance:**
- ‚úÖ snake_case naming for tables (plural) and columns
- ‚úÖ Portuguese for domain fields (tema, foco, enunciado, etc.)
- ‚úÖ English for technical fields (status, created_at, modelo_llm, etc.)
- ‚úÖ Follows transaction patterns from architecture doc
- ‚úÖ No anti-patterns detected

### File List

**Arquivos Criados:**
- src/construtor/metrics/store.py (CRIADO - 615 lines, MetricsStore implementation)
- tests/test_metrics/test_store.py (CRIADO - 547 lines, 27 comprehensive tests)
- tests/test_metrics/__init__.py (CRIADO)

**Arquivos Modificados:**
- src/construtor/metrics/__init__.py (MODIFICADO - export MetricsStore)
- .ruff.toml (MODIFICADO - added TRY003, EM101, EM102 to src/ ignores for context-rich exception messages)
- _bmad-output/implementation-artifacts/sprint-status.yaml (MODIFICADO - updated story 1-4 status to 'review')

**Arquivos Reutilizados:**
- src/construtor/models/question.py (REUTILIZADO - QuestionRecord, CriadorOutput)
- src/construtor/models/metrics.py (REUTILIZADO - QuestionMetrics)
- src/construtor/models/pipeline.py (REUTILIZADO - CheckpointResult, BatchState)
- src/construtor/config/exceptions.py (REUTILIZADO - PipelineError)

**Arquivos Verificados:**
- .gitignore (VERIFICADO - output/ j√° estava configurado)

---

**Created:** 2026-02-07
**Epic:** 1 - Funda√ß√£o do Projeto e Infraestrutura Core
**Priority:** CRITICAL - Bloqueia Stories 1.5, 1.6, todo Epic 2 (Pipeline), todo Epic 3 (Produ√ß√£o), todo Epic 4 (Dashboard)
**Estimated Effort:** 3-4 horas

